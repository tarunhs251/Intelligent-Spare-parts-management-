{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cedbf060-d1fe-4df1-865b-c48fec8b6c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.10.8)\n",
      "Requirement already satisfied: seaborn in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: openpyxl in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.3.1)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy matplotlib seaborn openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50fefd5a-dc24-411a-a47c-e76f56c0ecd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FILE SHAPES AND BASIC INFO\n",
      "================================================================================\n",
      "\n",
      "1. Demand Data: (0, 81)\n",
      "2. Parts Master: (10, 18)\n",
      "3. Suppliers: (7, 10)\n",
      "4. Inventory (XLSX): (7850, 10)\n",
      "5. Inventory (CSV): (7850, 10)\n",
      "6. Sales with Stock: (9500, 13)\n"
     ]
    }
   ],
   "source": [
    "# Loading and inspecting all uploaded files\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set styling\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.dpi'] = 150\n",
    "\n",
    "# Load all files\n",
    "files = {\n",
    "    'demand': 'model_ready_demand_data.csv',\n",
    "    'parts_master': 'parts_master_supplier_adjusted_10sku.csv',\n",
    "    'suppliers': 'suppliers_enhanced_generated.csv',\n",
    "    'inventory_xlsx': 'inventory_10sku.xlsx',\n",
    "    'inventory_csv': 'inventory_10sku.csv',\n",
    "    'sales_stock': 'sales_with_stock_10sku (1).csv'\n",
    "}\n",
    "\n",
    "# Load each file\n",
    "df_demand = pd.read_csv(files['demand'])\n",
    "df_parts = pd.read_csv(files['parts_master'])\n",
    "df_suppliers = pd.read_csv(files['suppliers'])\n",
    "df_inventory_xlsx = pd.read_excel(files['inventory_xlsx'])\n",
    "df_inventory_csv = pd.read_csv(files['inventory_csv'])\n",
    "df_sales = pd.read_csv(files['sales_stock'])\n",
    "\n",
    "# Display basic info\n",
    "print(\"=\" * 80)\n",
    "print(\"FILE SHAPES AND BASIC INFO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n1. Demand Data: {df_demand.shape}\")\n",
    "print(f\"2. Parts Master: {df_parts.shape}\")\n",
    "print(f\"3. Suppliers: {df_suppliers.shape}\")\n",
    "print(f\"4. Inventory (XLSX): {df_inventory_xlsx.shape}\")\n",
    "print(f\"5. Inventory (CSV): {df_inventory_csv.shape}\")\n",
    "print(f\"6. Sales with Stock: {df_sales.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a6d8fd3-5995-4c13-ad36-8c5b069b89fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "1. DEMAND DATA\n",
      "================================================================================\n",
      "Columns: ['transaction_id', 'date', 'part_sku', 'quantity_sold', 'quantity_in_hand', 'gst_rate_x', 'is_holiday', 'is_event', 'unit_price_after_gst', 'unit_cost_x', 'total_sales', 'quantity_in_hand_after_sales', 'quantity_on_hand', 'quantity_reserved', 'quantity_available', 'reorder_point', 'reorder_quantity', 'last_replenishment_date', 'days_since_replenishment', 'description', 'vehicle_make', 'vehicle_model', 'model_year_start', 'model_year_end', 'unit_cost_y', 'lifecycle_status', 'supersession_sku', 'gst_rate_y', 'pack_size', 'pack_cost', 'critical_spare', 'score_norm_x', 'supplier_variation_factor', 'unit_cost_new', 'supplier_name', 'avg_lead_time_days', 'lead_time_std_dev', 'supplier_rating', 'reliability_score', 'quality_score', 'cost_index', 'combined_score', 'score_norm_y', 'day_of_week', 'day', 'week', 'month', 'quarter', 'year', 'is_weekend', 'lag_1', 'lag_7', 'lag_14', 'lag_30', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'is_demand', 'zero_demand_ratio_30', 'days_since_last_sale', 'stockout_flag', 'days_of_cover', 'category_Brake System', 'category_Cooling System', 'category_Electrical', 'category_Engine Components', 'category_Suspension', 'category_Transmission', 'location_id_LOC_002', 'location_id_LOC_003', 'location_id_LOC_004', 'location_id_LOC_005', 'supplier_id_SUP_002', 'supplier_id_SUP_003', 'supplier_id_SUP_004', 'supplier_id_SUP_005', 'supplier_id_SUP_006', 'supplier_id_SUP_007']\n",
      "Note: This dataset is EMPTY (0 rows)\n",
      "\n",
      "================================================================================\n",
      "2. PARTS MASTER\n",
      "================================================================================\n",
      "      part_sku             description           category   vehicle_make  \\\n",
      "0  SKU_BRK_001     Brake Pad Set Front       Brake System  Maruti Suzuki   \n",
      "1  SKU_BRK_002        Brake Disc Front       Brake System  Maruti Suzuki   \n",
      "2  SKU_ENG_001       Engine Oil Filter  Engine Components        Hyundai   \n",
      "3  SKU_ENG_002              Air Filter  Engine Components        Hyundai   \n",
      "4  SKU_SUS_001  Suspension Strut Front         Suspension           Tata   \n",
      "\n",
      "        vehicle_model  model_year_start  model_year_end  unit_cost  \\\n",
      "0  Swift/Baleno/Dzire              2015            2024        800   \n",
      "1        Swift/Baleno              2015            2024       2200   \n",
      "2           i20/Creta              2016            2024        180   \n",
      "3     i20/Creta/Venue              2016            2024        320   \n",
      "4       Nexon/Harrier              2017            2024       3500   \n",
      "\n",
      "  supplier_id lifecycle_status  supersession_sku  gst_rate  pack_size  \\\n",
      "0     SUP_003           Active               NaN      0.28          4   \n",
      "1     SUP_003           Active               NaN      0.28          2   \n",
      "2     SUP_001           Active               NaN      0.18         10   \n",
      "3     SUP_001           Active               NaN      0.18          5   \n",
      "4     SUP_005           Active               NaN      0.28          2   \n",
      "\n",
      "   pack_cost  critical_spare  score_norm  supplier_variation_factor  \\\n",
      "0       3000           False        0.85                       1.05   \n",
      "1       4200            True        0.72                       1.08   \n",
      "2       1700           False        0.92                       1.02   \n",
      "3       1500           False        0.88                       1.03   \n",
      "4       6800            True        0.68                       1.12   \n",
      "\n",
      "   unit_cost_new  \n",
      "0            840  \n",
      "1           2376  \n",
      "2            183  \n",
      "3            329  \n",
      "4           3920  \n",
      "\n",
      "Columns: ['part_sku', 'description', 'category', 'vehicle_make', 'vehicle_model', 'model_year_start', 'model_year_end', 'unit_cost', 'supplier_id', 'lifecycle_status', 'supersession_sku', 'gst_rate', 'pack_size', 'pack_cost', 'critical_spare', 'score_norm', 'supplier_variation_factor', 'unit_cost_new']\n",
      "\n",
      "Data types:\n",
      "part_sku                      object\n",
      "description                   object\n",
      "category                      object\n",
      "vehicle_make                  object\n",
      "vehicle_model                 object\n",
      "model_year_start               int64\n",
      "model_year_end                 int64\n",
      "unit_cost                      int64\n",
      "supplier_id                   object\n",
      "lifecycle_status              object\n",
      "supersession_sku             float64\n",
      "gst_rate                     float64\n",
      "pack_size                      int64\n",
      "pack_cost                      int64\n",
      "critical_spare                  bool\n",
      "score_norm                   float64\n",
      "supplier_variation_factor    float64\n",
      "unit_cost_new                  int64\n",
      "dtype: object\n",
      "\n",
      "================================================================================\n",
      "3. SUPPLIERS\n",
      "================================================================================\n",
      "  supplier_id            supplier_name  avg_lead_time_days  lead_time_std_dev  \\\n",
      "0     SUP_001   AutoParts Direct India                   7                1.8   \n",
      "1     SUP_002  PowerCell Batteries Ltd                  10                2.5   \n",
      "2     SUP_003        BrakeTech Systems                   6                1.2   \n",
      "3     SUP_004     SparkLife Components                   8                2.0   \n",
      "4     SUP_005      SuspensionPro India                  12                3.0   \n",
      "\n",
      "   supplier_rating  reliability_score  quality_score  cost_index  \\\n",
      "0              4.5               0.92           0.94        1.05   \n",
      "1              4.2               0.88           0.90        1.08   \n",
      "2              4.7               0.95           0.96        1.12   \n",
      "3              4.3               0.90           0.88        1.03   \n",
      "4              4.0               0.85           0.87        1.15   \n",
      "\n",
      "   combined_score  score_norm  \n",
      "0            0.93        0.95  \n",
      "1            0.89        0.88  \n",
      "2            0.94        0.97  \n",
      "3            0.90        0.90  \n",
      "4            0.86        0.82  \n",
      "\n",
      "Columns: ['supplier_id', 'supplier_name', 'avg_lead_time_days', 'lead_time_std_dev', 'supplier_rating', 'reliability_score', 'quality_score', 'cost_index', 'combined_score', 'score_norm']\n"
     ]
    }
   ],
   "source": [
    "# Examining structure of each dataset\n",
    "\n",
    "# Examine each dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"1. DEMAND DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Columns: {list(df_demand.columns)}\")\n",
    "print(f\"Note: This dataset is EMPTY (0 rows)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. PARTS MASTER\")\n",
    "print(\"=\" * 80)\n",
    "print(df_parts.head())\n",
    "print(f\"\\nColumns: {list(df_parts.columns)}\")\n",
    "print(f\"\\nData types:\\n{df_parts.dtypes}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. SUPPLIERS\")\n",
    "print(\"=\" * 80)\n",
    "print(df_suppliers.head())\n",
    "print(f\"\\nColumns: {list(df_suppliers.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7c6a76d-a39b-4af4-b417-943d2c29da49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4. INVENTORY DATA (using CSV version)\n",
      "================================================================================\n",
      "      part_sku location_id        date  quantity_on_hand  quantity_reserved  \\\n",
      "0  SKU_BRK_001     LOC_001  03-01-2022               162                 20   \n",
      "1  SKU_BRK_001     LOC_002  03-01-2022               153                 10   \n",
      "2  SKU_BRK_001     LOC_003  03-01-2022               177                 21   \n",
      "3  SKU_BRK_001     LOC_004  03-01-2022               190                 26   \n",
      "4  SKU_BRK_001     LOC_005  03-01-2022               203                 20   \n",
      "5  SKU_BRK_002     LOC_001  03-01-2022               136                  8   \n",
      "6  SKU_BRK_002     LOC_002  03-01-2022                96                  9   \n",
      "7  SKU_BRK_002     LOC_003  03-01-2022               124                  7   \n",
      "8  SKU_BRK_002     LOC_004  03-01-2022                81                  5   \n",
      "9  SKU_BRK_002     LOC_005  03-01-2022               132                  7   \n",
      "\n",
      "   quantity_available  reorder_point  reorder_quantity  \\\n",
      "0                 142             26               123   \n",
      "1                 143             26               232   \n",
      "2                 156             26               271   \n",
      "3                 164             26               297   \n",
      "4                 183             26               191   \n",
      "5                 128              7                69   \n",
      "6                  87              7               146   \n",
      "7                 117              7               133   \n",
      "8                  76              7                87   \n",
      "9                 125              7                71   \n",
      "\n",
      "  last_replenishment_date  days_since_replenishment  \n",
      "0              12-12-2021                        22  \n",
      "1              17-12-2021                        17  \n",
      "2              14-12-2021                        20  \n",
      "3              08-12-2021                        26  \n",
      "4              02-01-2022                         1  \n",
      "5              07-12-2021                        27  \n",
      "6              24-12-2021                        10  \n",
      "7              14-12-2021                        20  \n",
      "8              29-12-2021                         5  \n",
      "9              13-12-2021                        21  \n",
      "\n",
      "Columns: ['part_sku', 'location_id', 'date', 'quantity_on_hand', 'quantity_reserved', 'quantity_available', 'reorder_point', 'reorder_quantity', 'last_replenishment_date', 'days_since_replenishment']\n",
      "\n",
      "Data types:\n",
      "part_sku                    object\n",
      "location_id                 object\n",
      "date                        object\n",
      "quantity_on_hand             int64\n",
      "quantity_reserved            int64\n",
      "quantity_available           int64\n",
      "reorder_point                int64\n",
      "reorder_quantity             int64\n",
      "last_replenishment_date     object\n",
      "days_since_replenishment     int64\n",
      "dtype: object\n",
      "\n",
      "Date range: 01-01-2024 to 31-10-2022\n",
      "Unique SKUs: 10\n",
      "Unique Locations: 5\n",
      "\n",
      "================================================================================\n",
      "5. SALES WITH STOCK DATA\n",
      "================================================================================\n",
      "      transaction_id              date     part_sku  quantity_sold  \\\n",
      "0  TXN_20220101_0001  01-01-2022 10:17  SKU_BRK_001              1   \n",
      "1  TXN_20220101_0002  01-01-2022 11:17  SKU_ENG_001              1   \n",
      "2  TXN_20220101_0003  01-01-2022 11:23  SKU_WPR_001              1   \n",
      "3  TXN_20220101_0004  01-01-2022 12:32  SKU_ENG_001              1   \n",
      "4  TXN_20220101_0005  01-01-2022 20:21  SKU_ENG_001              2   \n",
      "5  TXN_20220102_0006  02-01-2022 11:25  SKU_BRK_001              1   \n",
      "6  TXN_20220102_0007  02-01-2022 12:11  SKU_WPR_001              1   \n",
      "7  TXN_20220103_0008  03-01-2022 08:14  SKU_ELC_001              1   \n",
      "8  TXN_20220103_0009  03-01-2022 10:50  SKU_ELC_002              1   \n",
      "9  TXN_20220103_0010  03-01-2022 10:53  SKU_ENG_001              1   \n",
      "\n",
      "  location_id  quantity_in_hand  gst_rate  is_holiday  is_event  \\\n",
      "0     LOC_001               171      0.28       False     False   \n",
      "1     LOC_003               141      0.18       False     False   \n",
      "2     LOC_004               191      0.18       False     False   \n",
      "3     LOC_001               140      0.18       False     False   \n",
      "4     LOC_001               139      0.18       False     False   \n",
      "5     LOC_002               134      0.28       False     False   \n",
      "6     LOC_003               167      0.18       False     False   \n",
      "7     LOC_004               128      0.28       False     False   \n",
      "8     LOC_004               154      0.18       False     False   \n",
      "9     LOC_001               134      0.18       False     False   \n",
      "\n",
      "   unit_price_after_gst  unit_cost  total_sales  quantity_in_hand_after_sales  \n",
      "0                  1291        840         1291                           170  \n",
      "1                   254        183          254                           140  \n",
      "2                   427        282          427                           190  \n",
      "3                   260        183          260                           139  \n",
      "4                   262        183          524                           137  \n",
      "5                  1159        840         1159                           133  \n",
      "6                   396        282          396                           166  \n",
      "7                  5113       3392         5113                           127  \n",
      "8                   763        540          763                           153  \n",
      "9                   276        183          276                           133  \n",
      "\n",
      "Columns: ['transaction_id', 'date', 'part_sku', 'quantity_sold', 'location_id', 'quantity_in_hand', 'gst_rate', 'is_holiday', 'is_event', 'unit_price_after_gst', 'unit_cost', 'total_sales', 'quantity_in_hand_after_sales']\n",
      "\n",
      "Data types:\n",
      "transaction_id                   object\n",
      "date                             object\n",
      "part_sku                         object\n",
      "quantity_sold                     int64\n",
      "location_id                      object\n",
      "quantity_in_hand                  int64\n",
      "gst_rate                        float64\n",
      "is_holiday                         bool\n",
      "is_event                           bool\n",
      "unit_price_after_gst              int64\n",
      "unit_cost                         int64\n",
      "total_sales                       int64\n",
      "quantity_in_hand_after_sales      int64\n",
      "dtype: object\n",
      "\n",
      "Date range: 01-01-2022 10:17 to 31-12-2024 18:47\n",
      "Unique SKUs: 10\n"
     ]
    }
   ],
   "source": [
    "# Examining inventory and sales datasets\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"4. INVENTORY DATA (using CSV version)\")\n",
    "print(\"=\" * 80)\n",
    "print(df_inventory_csv.head(10))\n",
    "print(f\"\\nColumns: {list(df_inventory_csv.columns)}\")\n",
    "print(f\"\\nData types:\\n{df_inventory_csv.dtypes}\")\n",
    "print(f\"\\nDate range: {df_inventory_csv['date'].min()} to {df_inventory_csv['date'].max()}\")\n",
    "print(f\"Unique SKUs: {df_inventory_csv['part_sku'].nunique()}\")\n",
    "print(f\"Unique Locations: {df_inventory_csv['location_id'].nunique()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. SALES WITH STOCK DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(df_sales.head(10))\n",
    "print(f\"\\nColumns: {list(df_sales.columns)}\")\n",
    "print(f\"\\nData types:\\n{df_sales.dtypes}\")\n",
    "print(f\"\\nDate range: {df_sales['date'].min()} to {df_sales['date'].max()}\")\n",
    "print(f\"Unique SKUs: {df_sales['part_sku'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbb7e2c7-107c-4bf8-8882-e4c0c8021926",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "4. INVENTORY DATA - Summary\n",
      "================================================================================\n",
      "Unique SKUs: 10\n",
      "Unique Locations: 5\n",
      "\n",
      "SKUs: ['SKU_BRK_001', 'SKU_BRK_002', 'SKU_CLG_001', 'SKU_CLT_001', 'SKU_ELC_001', 'SKU_ELC_002', 'SKU_ENG_001', 'SKU_ENG_002', 'SKU_SUS_001', 'SKU_WPR_001']\n",
      "\n",
      "Locations: ['LOC_001', 'LOC_002', 'LOC_003', 'LOC_004', 'LOC_005']\n",
      "\n",
      "================================================================================\n",
      "5. SALES WITH STOCK DATA\n",
      "================================================================================\n",
      "      transaction_id              date     part_sku  quantity_sold  \\\n",
      "0  TXN_20220101_0001  01-01-2022 10:17  SKU_BRK_001              1   \n",
      "1  TXN_20220101_0002  01-01-2022 11:17  SKU_ENG_001              1   \n",
      "2  TXN_20220101_0003  01-01-2022 11:23  SKU_WPR_001              1   \n",
      "3  TXN_20220101_0004  01-01-2022 12:32  SKU_ENG_001              1   \n",
      "4  TXN_20220101_0005  01-01-2022 20:21  SKU_ENG_001              2   \n",
      "5  TXN_20220102_0006  02-01-2022 11:25  SKU_BRK_001              1   \n",
      "6  TXN_20220102_0007  02-01-2022 12:11  SKU_WPR_001              1   \n",
      "7  TXN_20220103_0008  03-01-2022 08:14  SKU_ELC_001              1   \n",
      "8  TXN_20220103_0009  03-01-2022 10:50  SKU_ELC_002              1   \n",
      "9  TXN_20220103_0010  03-01-2022 10:53  SKU_ENG_001              1   \n",
      "\n",
      "  location_id  quantity_in_hand  gst_rate  is_holiday  is_event  \\\n",
      "0     LOC_001               171      0.28       False     False   \n",
      "1     LOC_003               141      0.18       False     False   \n",
      "2     LOC_004               191      0.18       False     False   \n",
      "3     LOC_001               140      0.18       False     False   \n",
      "4     LOC_001               139      0.18       False     False   \n",
      "5     LOC_002               134      0.28       False     False   \n",
      "6     LOC_003               167      0.18       False     False   \n",
      "7     LOC_004               128      0.28       False     False   \n",
      "8     LOC_004               154      0.18       False     False   \n",
      "9     LOC_001               134      0.18       False     False   \n",
      "\n",
      "   unit_price_after_gst  unit_cost  total_sales  quantity_in_hand_after_sales  \n",
      "0                  1291        840         1291                           170  \n",
      "1                   254        183          254                           140  \n",
      "2                   427        282          427                           190  \n",
      "3                   260        183          260                           139  \n",
      "4                   262        183          524                           137  \n",
      "5                  1159        840         1159                           133  \n",
      "6                   396        282          396                           166  \n",
      "7                  5113       3392         5113                           127  \n",
      "8                   763        540          763                           153  \n",
      "9                   276        183          276                           133  \n",
      "\n",
      "Columns: ['transaction_id', 'date', 'part_sku', 'quantity_sold', 'location_id', 'quantity_in_hand', 'gst_rate', 'is_holiday', 'is_event', 'unit_price_after_gst', 'unit_cost', 'total_sales', 'quantity_in_hand_after_sales']\n",
      "\n",
      "Data types:\n",
      "transaction_id                   object\n",
      "date                             object\n",
      "part_sku                         object\n",
      "quantity_sold                     int64\n",
      "location_id                      object\n",
      "quantity_in_hand                  int64\n",
      "gst_rate                        float64\n",
      "is_holiday                         bool\n",
      "is_event                           bool\n",
      "unit_price_after_gst              int64\n",
      "unit_cost                         int64\n",
      "total_sales                       int64\n",
      "quantity_in_hand_after_sales      int64\n",
      "dtype: object\n",
      "\n",
      "Date range: 01-01-2022 10:17 to 31-12-2024 18:47\n",
      "Unique SKUs: 10\n",
      "\n",
      "SKUs: ['SKU_BRK_001', 'SKU_BRK_002', 'SKU_CLG_001', 'SKU_CLT_001', 'SKU_ELC_001', 'SKU_ELC_002', 'SKU_ENG_001', 'SKU_ENG_002', 'SKU_SUS_001', 'SKU_WPR_001']\n"
     ]
    }
   ],
   "source": [
    "# Correcting column names and examining datasets\n",
    "\n",
    "# Correct column name for inventory\n",
    "print(\"=\" * 80)\n",
    "print(\"4. INVENTORY DATA - Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Unique SKUs: {df_inventory_csv['part_sku'].nunique()}\")\n",
    "print(f\"Unique Locations: {df_inventory_csv['location_id'].nunique()}\")\n",
    "print(f\"\\nSKUs: {sorted(df_inventory_csv['part_sku'].unique())}\")\n",
    "print(f\"\\nLocations: {sorted(df_inventory_csv['location_id'].unique())}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. SALES WITH STOCK DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(df_sales.head(10))\n",
    "print(f\"\\nColumns: {list(df_sales.columns)}\")\n",
    "print(f\"\\nData types:\\n{df_sales.dtypes}\")\n",
    "print(f\"\\nDate range: {df_sales['date'].min()} to {df_sales['date'].max()}\")\n",
    "print(f\"Unique SKUs: {df_sales['part_sku'].nunique()}\")\n",
    "print(f\"\\nSKUs: {sorted(df_sales['part_sku'].unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1ec84e-0aec-49f4-a258-67c7a51a7bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SALES DATA - Summary\n",
      "================================================================================\n",
      "Date range: 2022-01-01 to 2024-12-31\n",
      "Total transactions: 9,500\n",
      "Unique SKUs: 10\n",
      "Unique Locations: 5\n",
      "\n",
      "Total quantity sold: 11,953\n",
      "Total sales value: ₹14,396,230.00\n",
      "\n",
      "Average transaction value: ₹1515.39\n",
      "Average quantity per transaction: 1.26\n",
      "\n",
      "================================================================================\n",
      "INVENTORY DATA - Summary\n",
      "================================================================================\n",
      "Date range: 2022-01-03 to 2024-12-30\n",
      "Total snapshots: 7,850\n",
      "Unique dates: 157\n",
      "\n",
      "================================================================================\n",
      "MISSING VALUES CHECK\n",
      "================================================================================\n",
      "\n",
      "Sales Data:\n",
      "transaction_id                  0\n",
      "date                            0\n",
      "part_sku                        0\n",
      "quantity_sold                   0\n",
      "location_id                     0\n",
      "quantity_in_hand                0\n",
      "gst_rate                        0\n",
      "is_holiday                      0\n",
      "is_event                        0\n",
      "unit_price_after_gst            0\n",
      "unit_cost                       0\n",
      "total_sales                     0\n",
      "quantity_in_hand_after_sales    0\n",
      "dtype: int64\n",
      "\n",
      "Inventory Data:\n",
      "part_sku                    0\n",
      "location_id                 0\n",
      "date                        0\n",
      "quantity_on_hand            0\n",
      "quantity_reserved           0\n",
      "quantity_available          0\n",
      "reorder_point               0\n",
      "reorder_quantity            0\n",
      "last_replenishment_date     0\n",
      "days_since_replenishment    0\n",
      "dtype: int64\n",
      "\n",
      "Parts Master:\n",
      "part_sku                      0\n",
      "description                   0\n",
      "category                      0\n",
      "vehicle_make                  0\n",
      "vehicle_model                 0\n",
      "model_year_start              0\n",
      "model_year_end                0\n",
      "unit_cost                     0\n",
      "supplier_id                   0\n",
      "lifecycle_status              0\n",
      "supersession_sku             10\n",
      "gst_rate                      0\n",
      "pack_size                     0\n",
      "pack_cost                     0\n",
      "critical_spare                0\n",
      "score_norm                    0\n",
      "supplier_variation_factor     0\n",
      "unit_cost_new                 0\n",
      "dtype: int64\n",
      "\n",
      "Suppliers:\n",
      "supplier_id           0\n",
      "supplier_name         0\n",
      "avg_lead_time_days    0\n",
      "lead_time_std_dev     0\n",
      "supplier_rating       0\n",
      "reliability_score     0\n",
      "quality_score         0\n",
      "cost_index            0\n",
      "combined_score        0\n",
      "score_norm            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Extracting date from timestamp and basic statistics\n",
    "\n",
    "# Extract date from timestamp\n",
    "#df_sales['date'] = pd.to_datetime(df_sales['date']).dt.date\n",
    "#df_inventory_csv['date'] = pd.to_datetime(df_inventory_csv['date'], format='%d-%m-%Y').dt.date\n",
    "# Extracting date from timestamp and basic statistics\n",
    "\n",
    "df_sales['date'] = pd.to_datetime(\n",
    "    df_sales['date'],\n",
    "    format='mixed',\n",
    "    dayfirst=True,\n",
    "    errors='coerce'\n",
    ").dt.date\n",
    "\n",
    "df_inventory_csv['date'] = pd.to_datetime(\n",
    "    df_inventory_csv['date'],\n",
    "    format='%d-%m-%Y',\n",
    "    errors='coerce'\n",
    ").dt.date\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SALES DATA - Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Date range: {df_sales['date'].min()} to {df_sales['date'].max()}\")\n",
    "print(f\"Total transactions: {len(df_sales):,}\")\n",
    "print(f\"Unique SKUs: {df_sales['part_sku'].nunique()}\")\n",
    "print(f\"Unique Locations: {df_sales['location_id'].nunique()}\")\n",
    "print(f\"\\nTotal quantity sold: {df_sales['quantity_sold'].sum():,}\")\n",
    "print(f\"Total sales value: ₹{df_sales['total_sales'].sum():,.2f}\")\n",
    "print(f\"\\nAverage transaction value: ₹{df_sales['total_sales'].mean():.2f}\")\n",
    "print(f\"Average quantity per transaction: {df_sales['quantity_sold'].mean():.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INVENTORY DATA - Summary\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Date range: {df_inventory_csv['date'].min()} to {df_inventory_csv['date'].max()}\")\n",
    "print(f\"Total snapshots: {len(df_inventory_csv):,}\")\n",
    "print(f\"Unique dates: {df_inventory_csv['date'].nunique()}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"MISSING VALUES CHECK\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nSales Data:\")\n",
    "print(df_sales.isnull().sum())\n",
    "print(\"\\nInventory Data:\")\n",
    "print(df_inventory_csv.isnull().sum())\n",
    "print(\"\\nParts Master:\")\n",
    "print(df_parts.isnull().sum())\n",
    "print(\"\\nSuppliers:\")\n",
    "print(df_suppliers.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7b137f-88ad-4363-96b1-90277df1491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged dataset shape: (9500, 39)\n",
      "\n",
      "Columns after merge:\n",
      "['transaction_id', 'date', 'part_sku', 'quantity_sold', 'location_id', 'quantity_in_hand', 'gst_rate', 'is_holiday', 'is_event', 'unit_price_after_gst', 'unit_cost', 'total_sales', 'quantity_in_hand_after_sales', 'description', 'category', 'vehicle_make', 'vehicle_model', 'model_year_start', 'model_year_end', 'unit_cost_parts', 'supplier_id', 'lifecycle_status', 'supersession_sku', 'gst_rate_parts', 'pack_size', 'pack_cost', 'critical_spare', 'score_norm', 'supplier_variation_factor', 'unit_cost_new', 'supplier_name', 'avg_lead_time_days', 'lead_time_std_dev', 'supplier_rating', 'reliability_score', 'quality_score', 'cost_index', 'combined_score', 'score_norm_supplier']\n",
      "\n",
      "Duplicate columns: []\n",
      "\n",
      "Sample of merged data:\n",
      "      transaction_id       date     part_sku  quantity_sold location_id  \\\n",
      "0  TXN_20220101_0001 2022-01-01  SKU_BRK_001              1     LOC_001   \n",
      "1  TXN_20220101_0002 2022-01-01  SKU_ENG_001              1     LOC_003   \n",
      "2  TXN_20220101_0003 2022-01-01  SKU_WPR_001              1     LOC_004   \n",
      "3  TXN_20220101_0004 2022-01-01  SKU_ENG_001              1     LOC_001   \n",
      "4  TXN_20220101_0005 2022-01-01  SKU_ENG_001              2     LOC_001   \n",
      "\n",
      "   quantity_in_hand  gst_rate  is_holiday  is_event  unit_price_after_gst  \\\n",
      "0               171      0.28       False     False                  1291   \n",
      "1               141      0.18       False     False                   254   \n",
      "2               191      0.18       False     False                   427   \n",
      "3               140      0.18       False     False                   260   \n",
      "4               139      0.18       False     False                   262   \n",
      "\n",
      "   ...  unit_cost_new           supplier_name  avg_lead_time_days  \\\n",
      "0  ...            840       BrakeTech Systems                   6   \n",
      "1  ...            183  AutoParts Direct India                   7   \n",
      "2  ...            282      VisionClear Wipers                   5   \n",
      "3  ...            183  AutoParts Direct India                   7   \n",
      "4  ...            183  AutoParts Direct India                   7   \n",
      "\n",
      "  lead_time_std_dev supplier_rating reliability_score quality_score  \\\n",
      "0               1.2             4.7              0.95          0.96   \n",
      "1               1.8             4.5              0.92          0.94   \n",
      "2               1.0             4.6              0.93          0.92   \n",
      "3               1.8             4.5              0.92          0.94   \n",
      "4               1.8             4.5              0.92          0.94   \n",
      "\n",
      "   cost_index  combined_score  score_norm_supplier  \n",
      "0        1.12            0.94                 0.97  \n",
      "1        1.05            0.93                 0.95  \n",
      "2        1.01            0.93                 0.94  \n",
      "3        1.05            0.93                 0.95  \n",
      "4        1.05            0.93                 0.95  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creating comprehensive merged dataset\n",
    "\n",
    "# Since demand data is empty, we'll build from sales data\n",
    "# Convert dates properly\n",
    "df_sales['date'] = pd.to_datetime(df_sales['date'])\n",
    "df_inventory_csv['date'] = pd.to_datetime(df_inventory_csv['date'], format='%d-%m-%Y')\n",
    "\n",
    "# Merge sales with parts master\n",
    "df_merged = df_sales.merge(df_parts, on='part_sku', how='left', suffixes=('', '_parts'))\n",
    "\n",
    "# Merge with suppliers\n",
    "df_merged = df_merged.merge(df_suppliers, on='supplier_id', how='left', suffixes=('', '_supplier'))\n",
    "\n",
    "print(\"Merged dataset shape:\", df_merged.shape)\n",
    "print(\"\\nColumns after merge:\")\n",
    "print(list(df_merged.columns))\n",
    "\n",
    "# Check for duplicates in column names\n",
    "print(f\"\\nDuplicate columns: {df_merged.columns[df_merged.columns.duplicated()].tolist()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of merged data:\")\n",
    "print(df_merged.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "047c6391-cc08-4694-8bc7-4a10c6297d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEMPORAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Sales by Year:\n",
      "      Total Quantity  Total Sales (₹)  Transactions\n",
      "year                                               \n",
      "2022            4066          5023487          3224\n",
      "2023            4068          4865469          3214\n",
      "2024            3819          4507274          3062\n",
      "\n",
      "Sales by Month:\n",
      "       Total Quantity  Total Sales (₹)  Transactions\n",
      "month                                               \n",
      "1                1096          1277552           864\n",
      "2                 906          1029676           710\n",
      "3                1108          1458576           890\n",
      "4                 943          1295247           755\n",
      "5                1010          1574466           801\n",
      "6                 933          1074976           750\n",
      "7                1017          1038388           782\n",
      "8                1026           976798           800\n",
      "9                 963           994338           757\n",
      "10                958          1147361           776\n",
      "11               1039          1217675           839\n",
      "12                954          1311177           776\n",
      "\n",
      "Sales by Day of Week:\n",
      "           Total Quantity  Total Sales (₹)  Transactions\n",
      "day_name                                                \n",
      "Monday               1900          2326788          1521\n",
      "Tuesday              2082          2508103          1662\n",
      "Wednesday            2144          2651452          1688\n",
      "Thursday             2122          2523490          1677\n",
      "Friday               1841          2223860          1478\n",
      "Saturday             1421          1624929          1108\n",
      "Sunday                443           537608           366\n",
      "\n",
      "Weekend vs Weekday:\n",
      "         Total Quantity  Total Sales (₹)  Transactions\n",
      "Weekday           10089         12233693          8026\n",
      "Weekend            1864          2162537          1474\n"
     ]
    }
   ],
   "source": [
    "# Performing comprehensive EDA - Part 1: Temporal patterns\n",
    "\n",
    "# Create a clean working dataset\n",
    "df = df_merged.copy()\n",
    "\n",
    "# Extract temporal features\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "df['day'] = df['date'].dt.day\n",
    "df['day_of_week'] = df['date'].dt.dayofweek\n",
    "df['day_name'] = df['date'].dt.day_name()\n",
    "df['week'] = df['date'].dt.isocalendar().week\n",
    "df['quarter'] = df['date'].dt.quarter\n",
    "df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "df['is_month_start'] = df['date'].dt.is_month_start.astype(int)\n",
    "df['is_month_end'] = df['date'].dt.is_month_end.astype(int)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TEMPORAL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sales by year\n",
    "print(\"\\nSales by Year:\")\n",
    "yearly_sales = df.groupby('year').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "yearly_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions']\n",
    "print(yearly_sales)\n",
    "\n",
    "# Sales by month\n",
    "print(\"\\nSales by Month:\")\n",
    "monthly_sales = df.groupby('month').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "monthly_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions']\n",
    "print(monthly_sales)\n",
    "\n",
    "# Sales by day of week\n",
    "print(\"\\nSales by Day of Week:\")\n",
    "dow_sales = df.groupby('day_name').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "dow_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions']\n",
    "# Reorder by day of week\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "dow_sales = dow_sales.reindex([d for d in day_order if d in dow_sales.index])\n",
    "print(dow_sales)\n",
    "\n",
    "# Weekend vs Weekday\n",
    "print(\"\\nWeekend vs Weekday:\")\n",
    "weekend_sales = df.groupby('is_weekend').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "weekend_sales.index = ['Weekday', 'Weekend']\n",
    "weekend_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions']\n",
    "print(weekend_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7af752b6-b4c8-4ce2-9f2d-4ad650863175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PRODUCT ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Sales by SKU:\n",
      "             Total Quantity  Total Sales (₹)  Transactions  Avg Price (₹)\n",
      "part_sku                                                                 \n",
      "SKU_ELC_001            1278          6197440          1034        4849.53\n",
      "SKU_BRK_001            2377          2851470          1873        1199.59\n",
      "SKU_ELC_002            1220           943712           965         773.62\n",
      "SKU_CLG_001            1199           823919           958         686.80\n",
      "SKU_ENG_002            1508           707682          1201         468.97\n",
      "SKU_WPR_001            1705           688181          1367         403.40\n",
      "SKU_ENG_001            2345           613395          1863         261.52\n",
      "SKU_CLT_001              82           568578            64        6928.88\n",
      "SKU_BRK_002             154           525874           119        3404.63\n",
      "SKU_SUS_001              85           475979            56        5611.09\n",
      "\n",
      "Sales by Category:\n",
      "                   Total Quantity  Total Sales (₹)  Transactions\n",
      "category                                                        \n",
      "Electrical                   2498          7141152          1999\n",
      "Brake System                 2531          3377344          1992\n",
      "Engine Components            3853          1321077          3064\n",
      "Cooling System               1199           823919           958\n",
      "Body/Exterior                1705           688181          1367\n",
      "Transmission                   82           568578            64\n",
      "Suspension                     85           475979            56\n",
      "\n",
      "Sales by Location:\n",
      "             Total Quantity  Total Sales (₹)  Transactions\n",
      "location_id                                               \n",
      "LOC_001                4248          5070107          3364\n",
      "LOC_002                2776          3479302          2221\n",
      "LOC_003                2413          2851810          1920\n",
      "LOC_004                1875          2183037          1478\n",
      "LOC_005                 641           811974           517\n",
      "\n",
      "Critical vs Non-Critical Spares:\n",
      "              Total Quantity  Total Sales (₹)  Transactions\n",
      "Non-Critical           11632         12825799          9261\n",
      "Critical                 321          1570431           239\n",
      "\n",
      "Sales by Supplier:\n",
      "                         Total Quantity  Total Sales (₹)  Transactions  \\\n",
      "supplier_name                                                            \n",
      "PowerCell Batteries Ltd            1278          6197440          1034   \n",
      "BrakeTech Systems                  2613          3945922          2056   \n",
      "AutoParts Direct India             3853          1321077          3064   \n",
      "SparkLife Components               1220           943712           965   \n",
      "CoolFlow Systems                   1199           823919           958   \n",
      "VisionClear Wipers                 1705           688181          1367   \n",
      "SuspensionPro India                  85           475979            56   \n",
      "\n",
      "                         Rating  Reliability  \n",
      "supplier_name                                 \n",
      "PowerCell Batteries Ltd     4.2         0.88  \n",
      "BrakeTech Systems           4.7         0.95  \n",
      "AutoParts Direct India      4.5         0.92  \n",
      "SparkLife Components        4.3         0.90  \n",
      "CoolFlow Systems            4.1         0.87  \n",
      "VisionClear Wipers          4.6         0.93  \n",
      "SuspensionPro India         4.0         0.85  \n"
     ]
    }
   ],
   "source": [
    "# EDA - Part 2: Product and location analysis\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PRODUCT ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Sales by SKU\n",
    "print(\"\\nSales by SKU:\")\n",
    "sku_sales = df.groupby('part_sku').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'unit_price_after_gst': 'mean'\n",
    "}).round(2)\n",
    "sku_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions', 'Avg Price (₹)']\n",
    "sku_sales = sku_sales.sort_values('Total Sales (₹)', ascending=False)\n",
    "print(sku_sales)\n",
    "\n",
    "# Sales by Category\n",
    "print(\"\\nSales by Category:\")\n",
    "cat_sales = df.groupby('category').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "cat_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions']\n",
    "cat_sales = cat_sales.sort_values('Total Sales (₹)', ascending=False)\n",
    "print(cat_sales)\n",
    "\n",
    "# Sales by Location\n",
    "print(\"\\nSales by Location:\")\n",
    "loc_sales = df.groupby('location_id').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "loc_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions']\n",
    "loc_sales = loc_sales.sort_values('Total Sales (₹)', ascending=False)\n",
    "print(loc_sales)\n",
    "\n",
    "# Critical spare analysis\n",
    "print(\"\\nCritical vs Non-Critical Spares:\")\n",
    "critical_sales = df.groupby('critical_spare').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "critical_sales.index = ['Non-Critical', 'Critical']\n",
    "critical_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions']\n",
    "print(critical_sales)\n",
    "\n",
    "# Supplier analysis\n",
    "print(\"\\nSales by Supplier:\")\n",
    "supplier_sales = df.groupby('supplier_name').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count',\n",
    "    'supplier_rating': 'first',\n",
    "    'reliability_score': 'first'\n",
    "}).round(2)\n",
    "supplier_sales.columns = ['Total Quantity', 'Total Sales (₹)', 'Transactions', 'Rating', 'Reliability']\n",
    "supplier_sales = supplier_sales.sort_values('Total Sales (₹)', ascending=False)\n",
    "print(supplier_sales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cab6d0d7-fddb-41dd-add5-e942e59d833c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STATISTICAL ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Quantity Sold Distribution:\n",
      "count    9500.000000\n",
      "mean        1.258211\n",
      "std         0.551735\n",
      "min         1.000000\n",
      "25%         1.000000\n",
      "50%         1.000000\n",
      "75%         1.000000\n",
      "max         7.000000\n",
      "Name: quantity_sold, dtype: float64\n",
      "\n",
      "Mode: 1\n",
      "Skewness: 2.22\n",
      "Kurtosis: 5.49\n",
      "\n",
      "Sales Value Distribution:\n",
      "count     9500.000000\n",
      "mean      1515.392632\n",
      "std       2186.377972\n",
      "min        244.000000\n",
      "25%        421.000000\n",
      "50%        742.000000\n",
      "75%       1258.000000\n",
      "max      39739.000000\n",
      "Name: total_sales, dtype: float64\n",
      "\n",
      "Skewness: 3.88\n",
      "Kurtosis: 25.50\n",
      "\n",
      "Inventory Levels (quantity_in_hand):\n",
      "count    9500.000000\n",
      "mean      127.868737\n",
      "std        72.283228\n",
      "min         1.000000\n",
      "25%        69.000000\n",
      "50%       117.000000\n",
      "75%       177.000000\n",
      "max       325.000000\n",
      "Name: quantity_in_hand, dtype: float64\n",
      "\n",
      "Price Analysis:\n",
      "count    9500.000000\n",
      "mean     1203.206105\n",
      "std      1493.133861\n",
      "min       244.000000\n",
      "25%       395.000000\n",
      "50%       668.500000\n",
      "75%      1182.250000\n",
      "max      7424.000000\n",
      "Name: unit_price_after_gst, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "CORRELATION ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Top correlations with quantity_sold:\n",
      "quantity_sold           1.000000\n",
      "total_sales             0.322674\n",
      "cost_index              0.015288\n",
      "reliability_score       0.005357\n",
      "quality_score           0.004804\n",
      "supplier_rating         0.004512\n",
      "unit_price_after_gst    0.001828\n",
      "unit_cost               0.001541\n",
      "avg_lead_time_days      0.000081\n",
      "quantity_in_hand       -0.009814\n",
      "Name: quantity_sold, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "HOLIDAY & EVENT IMPACT\n",
      "================================================================================\n",
      "\n",
      "Holiday Impact:\n",
      "            quantity_sold       total_sales          transaction_id\n",
      "                      sum  mean         sum     mean          count\n",
      "Non-Holiday         11933  1.26    14370110  1515.04           9485\n",
      "Holiday                20  1.33       26120  1741.33             15\n",
      "\n",
      "Event Impact:\n",
      "          quantity_sold       total_sales          transaction_id\n",
      "                    sum  mean         sum     mean          count\n",
      "Non-Event         11432  1.26    13822273  1521.94           9082\n",
      "Event               521  1.25      573957  1373.10            418\n"
     ]
    }
   ],
   "source": [
    "# EDA - Part 3: Statistical analysis and distributions\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STATISTICAL ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Quantity sold distribution\n",
    "print(\"\\nQuantity Sold Distribution:\")\n",
    "print(df['quantity_sold'].describe())\n",
    "print(f\"\\nMode: {df['quantity_sold'].mode()[0]}\")\n",
    "print(f\"Skewness: {df['quantity_sold'].skew():.2f}\")\n",
    "print(f\"Kurtosis: {df['quantity_sold'].kurtosis():.2f}\")\n",
    "\n",
    "# Sales value distribution\n",
    "print(\"\\nSales Value Distribution:\")\n",
    "print(df['total_sales'].describe())\n",
    "print(f\"\\nSkewness: {df['total_sales'].skew():.2f}\")\n",
    "print(f\"Kurtosis: {df['total_sales'].kurtosis():.2f}\")\n",
    "\n",
    "# Inventory levels\n",
    "print(\"\\nInventory Levels (quantity_in_hand):\")\n",
    "print(df['quantity_in_hand'].describe())\n",
    "\n",
    "# Price analysis\n",
    "print(\"\\nPrice Analysis:\")\n",
    "print(df['unit_price_after_gst'].describe())\n",
    "\n",
    "# Correlation analysis for numerical features\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "numerical_cols = ['quantity_sold', 'quantity_in_hand', 'unit_price_after_gst', \n",
    "                  'unit_cost', 'total_sales', 'avg_lead_time_days', 'supplier_rating',\n",
    "                  'reliability_score', 'quality_score', 'cost_index']\n",
    "\n",
    "corr_matrix = df[numerical_cols].corr()\n",
    "print(\"\\nTop correlations with quantity_sold:\")\n",
    "qty_corr = corr_matrix['quantity_sold'].sort_values(ascending=False)\n",
    "print(qty_corr)\n",
    "\n",
    "# Holiday and event impact\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"HOLIDAY & EVENT IMPACT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nHoliday Impact:\")\n",
    "holiday_impact = df.groupby('is_holiday').agg({\n",
    "    'quantity_sold': ['sum', 'mean'],\n",
    "    'total_sales': ['sum', 'mean'],\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "holiday_impact.index = ['Non-Holiday', 'Holiday']\n",
    "print(holiday_impact)\n",
    "\n",
    "print(\"\\nEvent Impact:\")\n",
    "event_impact = df.groupby('is_event').agg({\n",
    "    'quantity_sold': ['sum', 'mean'],\n",
    "    'total_sales': ['sum', 'mean'],\n",
    "    'transaction_id': 'count'\n",
    "}).round(2)\n",
    "event_impact.index = ['Non-Event', 'Event']\n",
    "print(event_impact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02ce9f04-51d1-4e73-a0b6-b2a811698e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eda_temporal_analysis.png\n"
     ]
    }
   ],
   "source": [
    "# Creating visualizations - Part 1: Temporal trends\n",
    "\n",
    "# Create comprehensive visualizations\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 14))\n",
    "\n",
    "# 1. Daily sales trend\n",
    "daily_sales = df.groupby('date')['total_sales'].sum().reset_index()\n",
    "axes[0, 0].plot(daily_sales['date'], daily_sales['total_sales'], linewidth=0.8, alpha=0.7)\n",
    "axes[0, 0].set_title('Daily Sales Trend', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Total Sales (₹)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Monthly sales trend\n",
    "monthly_trend = df.groupby([df['date'].dt.to_period('M')])['total_sales'].sum().reset_index()\n",
    "monthly_trend['date'] = monthly_trend['date'].dt.to_timestamp()\n",
    "axes[0, 1].plot(monthly_trend['date'], monthly_trend['total_sales'], marker='o', linewidth=2)\n",
    "axes[0, 1].set_title('Monthly Sales Trend', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Total Sales (₹)')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Sales by day of week\n",
    "dow_data = df.groupby('day_name')['total_sales'].sum().reindex(day_order)\n",
    "axes[1, 0].bar(range(len(dow_data)), dow_data.values, color='steelblue', alpha=0.7)\n",
    "axes[1, 0].set_xticks(range(len(dow_data)))\n",
    "axes[1, 0].set_xticklabels(dow_data.index, rotation=45)\n",
    "axes[1, 0].set_title('Sales by Day of Week', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Total Sales (₹)')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Sales by category\n",
    "cat_data = df.groupby('category')['total_sales'].sum().sort_values(ascending=True)\n",
    "axes[1, 1].barh(range(len(cat_data)), cat_data.values, color='coral', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(cat_data)))\n",
    "axes[1, 1].set_yticklabels(cat_data.index)\n",
    "axes[1, 1].set_title('Sales by Category', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Total Sales (₹)')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 5. Top 10 SKUs by sales\n",
    "sku_data = df.groupby('part_sku')['total_sales'].sum().sort_values(ascending=True).tail(10)\n",
    "axes[2, 0].barh(range(len(sku_data)), sku_data.values, color='mediumseagreen', alpha=0.7)\n",
    "axes[2, 0].set_yticks(range(len(sku_data)))\n",
    "axes[2, 0].set_yticklabels(sku_data.index)\n",
    "axes[2, 0].set_title('Top 10 SKUs by Sales', fontsize=12, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Total Sales (₹)')\n",
    "axes[2, 0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 6. Sales by location\n",
    "loc_data = df.groupby('location_id')['total_sales'].sum().sort_values(ascending=False)\n",
    "axes[2, 1].bar(range(len(loc_data)), loc_data.values, color='mediumpurple', alpha=0.7)\n",
    "axes[2, 1].set_xticks(range(len(loc_data)))\n",
    "axes[2, 1].set_xticklabels(loc_data.index, rotation=45)\n",
    "axes[2, 1].set_title('Sales by Location', fontsize=12, fontweight='bold')\n",
    "axes[2, 1].set_ylabel('Total Sales (₹)')\n",
    "axes[2, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_temporal_analysis.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved: eda_temporal_analysis.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36e7fa30-220f-419f-b37a-296b9a84e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: eda_distributions.png\n"
     ]
    }
   ],
   "source": [
    "# Creating visualizations - Part 2: Distributions and patterns\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Quantity sold distribution\n",
    "axes[0, 0].hist(df['quantity_sold'], bins=20, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Quantity Sold Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Quantity Sold')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 2. Sales value distribution (log scale)\n",
    "axes[0, 1].hist(np.log1p(df['total_sales']), bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_title('Sales Value Distribution (Log Scale)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Log(Total Sales)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 3. Inventory levels distribution\n",
    "axes[0, 2].hist(df['quantity_in_hand'], bins=30, color='mediumseagreen', alpha=0.7, edgecolor='black')\n",
    "axes[0, 2].set_title('Inventory Levels Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Quantity in Hand')\n",
    "axes[0, 2].set_ylabel('Frequency')\n",
    "axes[0, 2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# 4. Hourly sales pattern\n",
    "df['hour'] = pd.to_datetime(df['date']).dt.hour\n",
    "hourly_sales = df.groupby('hour')['transaction_id'].count()\n",
    "axes[1, 0].plot(hourly_sales.index, hourly_sales.values, marker='o', linewidth=2, color='steelblue')\n",
    "axes[1, 0].set_title('Hourly Transaction Pattern', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Hour of Day')\n",
    "axes[1, 0].set_ylabel('Number of Transactions')\n",
    "axes[1, 0].set_xticks(range(0, 24, 2))\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Supplier performance\n",
    "supplier_perf = df.groupby('supplier_name').agg({\n",
    "    'supplier_rating': 'first',\n",
    "    'total_sales': 'sum'\n",
    "}).sort_values('total_sales', ascending=False)\n",
    "axes[1, 1].scatter(supplier_perf['supplier_rating'], supplier_perf['total_sales'], \n",
    "                   s=200, alpha=0.6, c=range(len(supplier_perf)), cmap='viridis')\n",
    "for idx, (name, row) in enumerate(supplier_perf.iterrows()):\n",
    "    axes[1, 1].annotate(name.split()[0], (row['supplier_rating'], row['total_sales']), \n",
    "                       fontsize=8, ha='center')\n",
    "axes[1, 1].set_title('Supplier Rating vs Sales', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Supplier Rating')\n",
    "axes[1, 1].set_ylabel('Total Sales (₹)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. Critical spare impact\n",
    "critical_data = df.groupby('critical_spare')['total_sales'].sum()\n",
    "axes[1, 2].pie(critical_data.values, labels=['Non-Critical', 'Critical'], \n",
    "               autopct='%1.1f%%', startangle=90, colors=['lightblue', 'salmon'])\n",
    "axes[1, 2].set_title('Sales: Critical vs Non-Critical', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_distributions.png', dpi=150, bbox_inches='tight')\n",
    "print(\"Saved: eda_distributions.png\")\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dca5ea4d-0e22-4e3a-89d1-1b12d7ff2bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FEATURE ENGINEERING\n",
      "================================================================================\n",
      "\n",
      "1. Creating advanced temporal features...\n",
      "   - Added cyclical encodings for month, day, day_of_week\n",
      "   - Added quarter/year start/end indicators\n",
      "   - Added days_from_start\n",
      "\n",
      "2. Creating lag features...\n",
      "   - Added lag features: lag_1, lag_7, lag_14, lag_30\n",
      "\n",
      "3. Creating rolling window features...\n",
      "   - Added rolling statistics (mean, std, max, min) for windows: 7, 14, 30 days\n",
      "\n",
      "Current shape: (9500, 77)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Part 1: Temporal features\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create a comprehensive feature-engineered dataset\n",
    "df_fe = df.copy()\n",
    "\n",
    "# 1. TEMPORAL FEATURES (already created, but let's add more)\n",
    "print(\"\\n1. Creating advanced temporal features...\")\n",
    "\n",
    "# Cyclical encoding for temporal features\n",
    "df_fe['month_sin'] = np.sin(2 * np.pi * df_fe['month'] / 12)\n",
    "df_fe['month_cos'] = np.cos(2 * np.pi * df_fe['month'] / 12)\n",
    "df_fe['day_sin'] = np.sin(2 * np.pi * df_fe['day'] / 31)\n",
    "df_fe['day_cos'] = np.cos(2 * np.pi * df_fe['day'] / 31)\n",
    "df_fe['dow_sin'] = np.sin(2 * np.pi * df_fe['day_of_week'] / 7)\n",
    "df_fe['dow_cos'] = np.cos(2 * np.pi * df_fe['day_of_week'] / 7)\n",
    "\n",
    "# Time-based features\n",
    "df_fe['days_from_start'] = (df_fe['date'] - df_fe['date'].min()).dt.days\n",
    "df_fe['is_quarter_start'] = df_fe['date'].dt.is_quarter_start.astype(int)\n",
    "df_fe['is_quarter_end'] = df_fe['date'].dt.is_quarter_end.astype(int)\n",
    "df_fe['is_year_start'] = df_fe['date'].dt.is_year_start.astype(int)\n",
    "df_fe['is_year_end'] = df_fe['date'].dt.is_year_end.astype(int)\n",
    "\n",
    "print(f\"   - Added cyclical encodings for month, day, day_of_week\")\n",
    "print(f\"   - Added quarter/year start/end indicators\")\n",
    "print(f\"   - Added days_from_start\")\n",
    "\n",
    "# 2. LAG FEATURES\n",
    "print(\"\\n2. Creating lag features...\")\n",
    "\n",
    "# Sort by part_sku, location, and date for proper lag calculation\n",
    "df_fe = df_fe.sort_values(['part_sku', 'location_id', 'date'])\n",
    "\n",
    "# Create lag features for quantity sold\n",
    "for lag in [1, 7, 14, 30]:\n",
    "    df_fe[f'lag_{lag}'] = df_fe.groupby(['part_sku', 'location_id'])['quantity_sold'].shift(lag)\n",
    "\n",
    "print(f\"   - Added lag features: lag_1, lag_7, lag_14, lag_30\")\n",
    "\n",
    "# 3. ROLLING STATISTICS\n",
    "print(\"\\n3. Creating rolling window features...\")\n",
    "\n",
    "# Rolling mean and std for different windows\n",
    "for window in [7, 14, 30]:\n",
    "    df_fe[f'rolling_mean_{window}'] = df_fe.groupby(['part_sku', 'location_id'])['quantity_sold'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "    )\n",
    "    df_fe[f'rolling_std_{window}'] = df_fe.groupby(['part_sku', 'location_id'])['quantity_sold'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "    )\n",
    "    df_fe[f'rolling_max_{window}'] = df_fe.groupby(['part_sku', 'location_id'])['quantity_sold'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).max()\n",
    "    )\n",
    "    df_fe[f'rolling_min_{window}'] = df_fe.groupby(['part_sku', 'location_id'])['quantity_sold'].transform(\n",
    "        lambda x: x.rolling(window=window, min_periods=1).min()\n",
    "    )\n",
    "\n",
    "print(f\"   - Added rolling statistics (mean, std, max, min) for windows: 7, 14, 30 days\")\n",
    "\n",
    "print(f\"\\nCurrent shape: {df_fe.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2023cb74-ddcc-44b1-935a-a7d3454f9a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Creating demand pattern features...\n",
      "   - Added demand volatility, trend, and zero-demand indicators\n",
      "\n",
      "5. Creating inventory features...\n",
      "   - Added inventory ratios, turnover, and stock level indicators\n",
      "\n",
      "6. Creating price and cost features...\n",
      "   - Added profit margin, markup ratio, and cost efficiency\n",
      "\n",
      "Current shape: (9500, 95)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Part 2: Demand and inventory features\n",
    "\n",
    "print(\"\\n4. Creating demand pattern features...\")\n",
    "\n",
    "# Demand variability\n",
    "df_fe['demand_volatility_7'] = df_fe['rolling_std_7'] / (df_fe['rolling_mean_7'] + 1)\n",
    "df_fe['demand_volatility_30'] = df_fe['rolling_std_30'] / (df_fe['rolling_mean_30'] + 1)\n",
    "\n",
    "# Demand trend\n",
    "df_fe['demand_trend_7_30'] = df_fe['rolling_mean_7'] / (df_fe['rolling_mean_30'] + 1)\n",
    "\n",
    "# Zero demand indicators\n",
    "df_fe['is_demand'] = (df_fe['quantity_sold'] > 0).astype(int)\n",
    "df_fe['zero_demand_ratio_7'] = df_fe.groupby(['part_sku', 'location_id'])['is_demand'].transform(\n",
    "    lambda x: 1 - x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "df_fe['zero_demand_ratio_30'] = df_fe.groupby(['part_sku', 'location_id'])['is_demand'].transform(\n",
    "    lambda x: 1 - x.rolling(window=30, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# Days since last sale\n",
    "df_fe['days_since_last_sale'] = df_fe.groupby(['part_sku', 'location_id']).cumcount()\n",
    "\n",
    "print(f\"   - Added demand volatility, trend, and zero-demand indicators\")\n",
    "\n",
    "print(\"\\n5. Creating inventory features...\")\n",
    "\n",
    "# Inventory ratios\n",
    "df_fe['inventory_to_sales_ratio'] = df_fe['quantity_in_hand'] / (df_fe['quantity_sold'] + 1)\n",
    "df_fe['inventory_turnover_7'] = df_fe['quantity_in_hand'] / (df_fe['rolling_mean_7'] + 1)\n",
    "df_fe['inventory_turnover_30'] = df_fe['quantity_in_hand'] / (df_fe['rolling_mean_30'] + 1)\n",
    "\n",
    "# Stock level indicators\n",
    "df_fe['low_stock_flag'] = (df_fe['quantity_in_hand'] < 50).astype(int)\n",
    "df_fe['high_stock_flag'] = (df_fe['quantity_in_hand'] > 200).astype(int)\n",
    "df_fe['stockout_flag'] = (df_fe['quantity_in_hand'] == 0).astype(int)\n",
    "\n",
    "# Days of cover (how many days current inventory can last)\n",
    "df_fe['days_of_cover'] = df_fe['quantity_in_hand'] / (df_fe['rolling_mean_7'] + 1)\n",
    "\n",
    "print(f\"   - Added inventory ratios, turnover, and stock level indicators\")\n",
    "\n",
    "print(\"\\n6. Creating price and cost features...\")\n",
    "\n",
    "# Price-related features\n",
    "df_fe['profit_margin'] = (df_fe['unit_price_after_gst'] - df_fe['unit_cost']) / df_fe['unit_price_after_gst']\n",
    "df_fe['markup_ratio'] = df_fe['unit_price_after_gst'] / (df_fe['unit_cost'] + 1)\n",
    "df_fe['revenue_per_unit'] = df_fe['total_sales'] / (df_fe['quantity_sold'] + 1)\n",
    "\n",
    "# Cost efficiency\n",
    "df_fe['cost_efficiency'] = df_fe['unit_cost_new'] / (df_fe['unit_cost'] + 1)\n",
    "\n",
    "print(f\"   - Added profit margin, markup ratio, and cost efficiency\")\n",
    "\n",
    "print(f\"\\nCurrent shape: {df_fe.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c2a1f47-096e-46b4-b88b-ef6ff32903db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Creating aggregated features by SKU and location...\n",
      "   - Added SKU-level, location-level, and SKU-location aggregations\n",
      "\n",
      "8. Creating interaction features...\n",
      "   - Added interaction features\n",
      "\n",
      "9. Creating supplier and quality features...\n",
      "   - Added supplier performance features\n",
      "\n",
      "Final shape: (9500, 117)\n",
      "Total features created: 117\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering - Part 3: Categorical and interaction features\n",
    "\n",
    "print(\"\\n7. Creating aggregated features by SKU and location...\")\n",
    "\n",
    "# SKU-level aggregations\n",
    "sku_agg = df_fe.groupby('part_sku').agg({\n",
    "    'quantity_sold': ['mean', 'std', 'sum'],\n",
    "    'total_sales': ['mean', 'sum'],\n",
    "    'transaction_id': 'count'\n",
    "}).reset_index()\n",
    "sku_agg.columns = ['part_sku', 'sku_avg_qty', 'sku_std_qty', 'sku_total_qty', \n",
    "                   'sku_avg_sales', 'sku_total_sales', 'sku_transaction_count']\n",
    "df_fe = df_fe.merge(sku_agg, on='part_sku', how='left')\n",
    "\n",
    "# Location-level aggregations\n",
    "loc_agg = df_fe.groupby('location_id').agg({\n",
    "    'quantity_sold': ['mean', 'std', 'sum'],\n",
    "    'total_sales': ['mean', 'sum'],\n",
    "    'transaction_id': 'count'\n",
    "}).reset_index()\n",
    "loc_agg.columns = ['location_id', 'loc_avg_qty', 'loc_std_qty', 'loc_total_qty',\n",
    "                   'loc_avg_sales', 'loc_total_sales', 'loc_transaction_count']\n",
    "df_fe = df_fe.merge(loc_agg, on='location_id', how='left')\n",
    "\n",
    "# SKU-Location combination aggregations\n",
    "sku_loc_agg = df_fe.groupby(['part_sku', 'location_id']).agg({\n",
    "    'quantity_sold': ['mean', 'std'],\n",
    "    'total_sales': 'mean'\n",
    "}).reset_index()\n",
    "sku_loc_agg.columns = ['part_sku', 'location_id', 'sku_loc_avg_qty', 'sku_loc_std_qty', 'sku_loc_avg_sales']\n",
    "df_fe = df_fe.merge(sku_loc_agg, on=['part_sku', 'location_id'], how='left')\n",
    "\n",
    "print(f\"   - Added SKU-level, location-level, and SKU-location aggregations\")\n",
    "\n",
    "print(\"\\n8. Creating interaction features...\")\n",
    "\n",
    "# Interaction features\n",
    "df_fe['qty_price_interaction'] = df_fe['quantity_sold'] * df_fe['unit_price_after_gst']\n",
    "df_fe['inventory_demand_interaction'] = df_fe['quantity_in_hand'] * df_fe['rolling_mean_7']\n",
    "df_fe['weekend_critical_interaction'] = df_fe['is_weekend'] * df_fe['critical_spare'].astype(int)\n",
    "df_fe['holiday_event_interaction'] = df_fe['is_holiday'].astype(int) * df_fe['is_event'].astype(int)\n",
    "\n",
    "print(f\"   - Added interaction features\")\n",
    "\n",
    "print(\"\\n9. Creating supplier and quality features...\")\n",
    "\n",
    "# Supplier performance features\n",
    "df_fe['supplier_lead_time_reliability'] = df_fe['avg_lead_time_days'] * df_fe['reliability_score']\n",
    "df_fe['supplier_quality_cost'] = df_fe['quality_score'] / (df_fe['cost_index'] + 0.1)\n",
    "df_fe['supplier_overall_score'] = (df_fe['reliability_score'] + df_fe['quality_score'] + \n",
    "                                    df_fe['supplier_rating']/5) / 3\n",
    "\n",
    "print(f\"   - Added supplier performance features\")\n",
    "\n",
    "print(f\"\\nFinal shape: {df_fe.shape}\")\n",
    "print(f\"Total features created: {df_fe.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d5a47b3-14cd-4961-9a4b-1ace30cae0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10. Encoding categorical variables...\n",
      "   - Encoded categorical variables: ['category', 'location_id', 'supplier_id', 'lifecycle_status']\n",
      "   - Shape after encoding: (9500, 129)\n",
      "   - Converted 19 boolean columns to integers\n",
      "\n",
      "11. Handling missing values...\n",
      "   - Missing values before: 12089\n",
      "   - Missing values after: 0\n",
      "\n",
      "Final dataset shape: (9500, 129)\n",
      "Total features: 129\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical variables and preparing final dataset\n",
    "\n",
    "print(\"\\n10. Encoding categorical variables...\")\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "categorical_cols = ['category', 'location_id', 'supplier_id', 'lifecycle_status']\n",
    "\n",
    "# Create dummy variables\n",
    "df_encoded = pd.get_dummies(df_fe, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"   - Encoded categorical variables: {categorical_cols}\")\n",
    "print(f\"   - Shape after encoding: {df_encoded.shape}\")\n",
    "\n",
    "# Convert boolean columns to int\n",
    "bool_cols = df_encoded.select_dtypes(include='bool').columns\n",
    "df_encoded[bool_cols] = df_encoded[bool_cols].astype(int)\n",
    "\n",
    "print(f\"   - Converted {len(bool_cols)} boolean columns to integers\")\n",
    "\n",
    "# Handle any remaining missing values\n",
    "print(\"\\n11. Handling missing values...\")\n",
    "missing_before = df_encoded.isnull().sum().sum()\n",
    "print(f\"   - Missing values before: {missing_before}\")\n",
    "\n",
    "# Fill missing values in lag and rolling features with 0 or forward fill\n",
    "lag_cols = [col for col in df_encoded.columns if 'lag_' in col or 'rolling_' in col]\n",
    "df_encoded[lag_cols] = df_encoded[lag_cols].fillna(0)\n",
    "\n",
    "# Fill any remaining missing values\n",
    "df_encoded = df_encoded.fillna(0)\n",
    "\n",
    "missing_after = df_encoded.isnull().sum().sum()\n",
    "print(f\"   - Missing values after: {missing_after}\")\n",
    "\n",
    "print(f\"\\nFinal dataset shape: {df_encoded.shape}\")\n",
    "print(f\"Total features: {df_encoded.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6aea78e-788b-4785-ac9c-ddae6662752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING DATASET FOR MODEL TRAINING\n",
      "================================================================================\n",
      "\n",
      "Target variable: quantity_sold\n",
      "Number of features: 117\n",
      "Number of samples: 9500\n",
      "\n",
      "Feature matrix shape: (9500, 117)\n",
      "Target vector shape: (9500,)\n",
      "\n",
      "================================================================================\n",
      "TRAIN-TEST SPLIT (Time-based)\n",
      "================================================================================\n",
      "Training set: 7600 samples (80.0%)\n",
      "Test set: 1900 samples (20.0%)\n",
      "\n",
      "Train date range: 2022-01-01 00:00:00 to 2024-05-15 00:00:00\n",
      "Test date range: 2024-05-15 00:00:00 to 2024-12-31 00:00:00\n",
      "\n",
      "================================================================================\n",
      "FEATURE CATEGORIES\n",
      "================================================================================\n",
      "Temporal features: 27\n",
      "Lag features: 4\n",
      "Rolling window features: 12\n",
      "Inventory features: 9\n",
      "Price/Cost features: 12\n",
      "Supplier features: 16\n",
      "Demand pattern features: 7\n",
      "Categorical features: 16\n",
      "Other features: 14\n"
     ]
    }
   ],
   "source": [
    "# Preparing train-test split and feature selection\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPARING DATASET FOR MODEL TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define target variable\n",
    "target = 'quantity_sold'\n",
    "\n",
    "# Define features to exclude from modeling\n",
    "exclude_cols = [\n",
    "    'transaction_id', 'timestamp', 'date', 'part_sku', 'description',\n",
    "    'vehicle_make', 'vehicle_model', 'day_name', 'supplier_name',\n",
    "    'total_sales',  # This is derived from quantity_sold\n",
    "    'quantity_in_hand_after_sales',  # This is post-transaction\n",
    "    'supersession_sku',  # Mostly null\n",
    "    target  # Our target variable\n",
    "]\n",
    "\n",
    "# Get feature columns\n",
    "feature_cols = [col for col in df_encoded.columns if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nTarget variable: {target}\")\n",
    "print(f\"Number of features: {len(feature_cols)}\")\n",
    "print(f\"Number of samples: {len(df_encoded)}\")\n",
    "\n",
    "# Create feature matrix and target vector\n",
    "X = df_encoded[feature_cols].copy()\n",
    "y = df_encoded[target].copy()\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "\n",
    "# Time-based split (80-20 split based on date)\n",
    "# Sort by date to ensure temporal ordering\n",
    "df_encoded_sorted = df_encoded.sort_values('date')\n",
    "split_idx = int(len(df_encoded_sorted) * 0.8)\n",
    "\n",
    "train_data = df_encoded_sorted.iloc[:split_idx]\n",
    "test_data = df_encoded_sorted.iloc[split_idx:]\n",
    "\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data[target]\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data[target]\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAIN-TEST SPLIT (Time-based)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(df_encoded)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(df_encoded)*100:.1f}%)\")\n",
    "print(f\"\\nTrain date range: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
    "print(f\"Test date range: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
    "\n",
    "# Display feature categories\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(\"FEATURE CATEGORIES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "temporal_features = [col for col in feature_cols if any(x in col for x in ['month', 'day', 'week', 'quarter', 'year', 'weekend', 'hour'])]\n",
    "lag_features = [col for col in feature_cols if 'lag_' in col]\n",
    "rolling_features = [col for col in feature_cols if 'rolling_' in col]\n",
    "inventory_features = [col for col in feature_cols if any(x in col for x in ['inventory', 'stock', 'quantity_in_hand', 'cover'])]\n",
    "price_features = [col for col in feature_cols if any(x in col for x in ['price', 'cost', 'margin', 'markup', 'revenue'])]\n",
    "supplier_features = [col for col in feature_cols if any(x in col for x in ['supplier', 'lead_time', 'reliability', 'quality', 'rating'])]\n",
    "demand_features = [col for col in feature_cols if any(x in col for x in ['demand', 'volatility', 'trend'])]\n",
    "categorical_features = [col for col in feature_cols if any(x in col for x in ['category_', 'location_id_', 'supplier_id_', 'lifecycle'])]\n",
    "\n",
    "print(f\"Temporal features: {len(temporal_features)}\")\n",
    "print(f\"Lag features: {len(lag_features)}\")\n",
    "print(f\"Rolling window features: {len(rolling_features)}\")\n",
    "print(f\"Inventory features: {len(inventory_features)}\")\n",
    "print(f\"Price/Cost features: {len(price_features)}\")\n",
    "print(f\"Supplier features: {len(supplier_features)}\")\n",
    "print(f\"Demand pattern features: {len(demand_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "print(f\"Other features: {len(feature_cols) - sum([len(temporal_features), len(lag_features), len(rolling_features), len(inventory_features), len(price_features), len(supplier_features), len(demand_features), len(categorical_features)])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5a7ec550-5bc5-43bb-bb3c-25377ed14839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAVING PROCESSED DATASETS\n",
      "================================================================================\n",
      "\n",
      "✓ Saved feature-engineered dataset: processed_data\\feature_engineered_dataset.csv\n",
      "  Shape: (9500, 117)\n",
      "  Size: 9.11 MB\n",
      "\n",
      "✓ Saved encoded dataset: processed_data\\encoded_dataset_model_ready.csv\n",
      "  Shape: (9500, 129)\n",
      "  Size: 8.99 MB\n",
      "\n",
      "✓ Saved training features: processed_data\\X_train.csv\n",
      "✓ Saved training target: processed_data\\y_train.csv\n",
      "✓ Saved test features: processed_data\\X_test.csv\n",
      "✓ Saved test target: processed_data\\y_test.csv\n",
      "\n",
      "✓ Saved feature information: processed_data\\feature_information.csv\n",
      "\n",
      "✓ Saved SKU statistics: processed_data\\sku_statistics.csv\n",
      "✓ Saved location statistics: processed_data\\location_statistics.csv\n",
      "✓ Saved SKU-location statistics: processed_data\\sku_location_statistics.csv\n",
      "\n",
      "================================================================================\n",
      "ALL DATASETS SAVED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Save processed datasets and create summary report\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory\n",
    "output_dir = 'processed_data'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SAVING PROCESSED DATASETS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Save feature-engineered dataset\n",
    "df_fe_to_save = df_fe.copy()\n",
    "output_file = os.path.join(output_dir, 'feature_engineered_dataset.csv')\n",
    "df_fe_to_save.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved feature-engineered dataset: {output_file}\")\n",
    "print(f\"  Shape: {df_fe_to_save.shape}\")\n",
    "print(f\"  Size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# 2. Save encoded dataset (model-ready)\n",
    "df_encoded_to_save = df_encoded.copy()\n",
    "output_file = os.path.join(output_dir, 'encoded_dataset_model_ready.csv')\n",
    "df_encoded_to_save.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved encoded dataset: {output_file}\")\n",
    "print(f\"  Shape: {df_encoded_to_save.shape}\")\n",
    "print(f\"  Size: {os.path.getsize(output_file) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# 3. Save train-test split datasets\n",
    "output_file = os.path.join(output_dir, 'X_train.csv')\n",
    "X_train.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved training features: {output_file}\")\n",
    "\n",
    "output_file = os.path.join(output_dir, 'y_train.csv')\n",
    "y_train.to_csv(output_file, index=False, header=['quantity_sold'])\n",
    "print(f\"✓ Saved training target: {output_file}\")\n",
    "\n",
    "output_file = os.path.join(output_dir, 'X_test.csv')\n",
    "X_test.to_csv(output_file, index=False)\n",
    "print(f\"✓ Saved test features: {output_file}\")\n",
    "\n",
    "output_file = os.path.join(output_dir, 'y_test.csv')\n",
    "y_test.to_csv(output_file, index=False, header=['quantity_sold'])\n",
    "print(f\"✓ Saved test target: {output_file}\")\n",
    "\n",
    "# 4. Save feature names for reference\n",
    "feature_info = pd.DataFrame({\n",
    "    'feature_name': feature_cols,\n",
    "    'feature_type': ['unknown'] * len(feature_cols)\n",
    "})\n",
    "\n",
    "# Categorize features\n",
    "for idx, col in enumerate(feature_cols):\n",
    "    if any(x in col for x in ['month', 'day', 'week', 'quarter', 'year', 'weekend', 'hour']):\n",
    "        feature_info.loc[idx, 'feature_type'] = 'temporal'\n",
    "    elif 'lag_' in col:\n",
    "        feature_info.loc[idx, 'feature_type'] = 'lag'\n",
    "    elif 'rolling_' in col:\n",
    "        feature_info.loc[idx, 'feature_type'] = 'rolling_window'\n",
    "    elif any(x in col for x in ['inventory', 'stock', 'quantity_in_hand', 'cover']):\n",
    "        feature_info.loc[idx, 'feature_type'] = 'inventory'\n",
    "    elif any(x in col for x in ['price', 'cost', 'margin', 'markup', 'revenue']):\n",
    "        feature_info.loc[idx, 'feature_type'] = 'price_cost'\n",
    "    elif any(x in col for x in ['supplier', 'lead_time', 'reliability', 'quality', 'rating']):\n",
    "        feature_info.loc[idx, 'feature_type'] = 'supplier'\n",
    "    elif any(x in col for x in ['demand', 'volatility', 'trend']):\n",
    "        feature_info.loc[idx, 'feature_type'] = 'demand_pattern'\n",
    "    elif any(x in col for x in ['category_', 'location_id_', 'supplier_id_', 'lifecycle']):\n",
    "        feature_info.loc[idx, 'feature_type'] = 'categorical'\n",
    "    else:\n",
    "        feature_info.loc[idx, 'feature_type'] = 'other'\n",
    "\n",
    "output_file = os.path.join(output_dir, 'feature_information.csv')\n",
    "feature_info.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved feature information: {output_file}\")\n",
    "\n",
    "# 5. Save aggregated statistics\n",
    "output_file = os.path.join(output_dir, 'sku_statistics.csv')\n",
    "sku_agg.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Saved SKU statistics: {output_file}\")\n",
    "\n",
    "output_file = os.path.join(output_dir, 'location_statistics.csv')\n",
    "loc_agg.to_csv(output_file, index=False)\n",
    "print(f\"✓ Saved location statistics: {output_file}\")\n",
    "\n",
    "output_file = os.path.join(output_dir, 'sku_location_statistics.csv')\n",
    "sku_loc_agg.to_csv(output_file, index=False)\n",
    "print(f\"✓ Saved SKU-location statistics: {output_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL DATASETS SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b71553a-e6e7-43af-95ab-01fbcb42527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING SUMMARY REPORT\n",
      "================================================================================\n",
      "================================================================================\n",
      "AUTOMOTIVE SPARE PARTS - EDA & FEATURE ENGINEERING REPORT\n",
      "================================================================================\n",
      "\n",
      "Report Generated: 2026-01-07 01:44:19\n",
      "\n",
      "================================================================================\n",
      "\n",
      "1. DATA OVERVIEW\n",
      "--------------------------------------------------------------------------------\n",
      "Total Transactions: 9,500\n",
      "Date Range: 2022-01-01 00:00:00 to 2024-12-31 00:00:00\n",
      "Number of SKUs: 10\n",
      "Number of Locations: 5\n",
      "Number of Suppliers: 7\n",
      "\n",
      "Total Quantity Sold: 11,953 units\n",
      "Total Sales Value: ₹14,396,230.00\n",
      "Average Transaction Value: ₹1515.39\n",
      "\n",
      "================================================================================\n",
      "\n",
      "2. TEMPORAL ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sales by Year:\n",
      "  2022: 4,066 units | ₹5,023,487 | 3,224 transactions\n",
      "  2023: 4,068 units | ₹4,865,469 | 3,214 transactions\n",
      "  2024: 3,819 units | ₹4,507,274 | 3,062 transactions\n",
      "\n",
      "Weekend vs Weekday Sales:\n",
      "  Weekday: ₹12,233,693 (85.0%)\n",
      "  Weekend: ₹2,162,537 (15.0%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "3. PRODUCT ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Top 5 SKUs by Sales Value:\n",
      "  1. SKU_ELC_001: ₹6,197,440\n",
      "  2. SKU_BRK_001: ₹2,851,470\n",
      "  3. SKU_ELC_002: ₹943,712\n",
      "  4. SKU_CLG_001: ₹823,919\n",
      "  5. SKU_ENG_002: ₹707,682\n",
      "\n",
      "Sales by Category:\n",
      "  Electrical: ₹7,141,152 (49.6%)\n",
      "  Brake System: ₹3,377,344 (23.5%)\n",
      "  Engine Components: ₹1,321,077 (9.2%)\n",
      "  Cooling System: ₹823,919 (5.7%)\n",
      "  Body/Exterior: ₹688,181 (4.8%)\n",
      "  Transmission: ₹568,578 (3.9%)\n",
      "  Suspension: ₹475,979 (3.3%)\n",
      "\n",
      "Critical vs Non-Critical Spares:\n",
      "  Non-Critical: ₹12,825,799 (89.1%)\n",
      "  Critical: ₹1,570,431 (10.9%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "4. LOCATION ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Sales by Location:\n",
      "  LOC_001: ₹5,070,107 (35.2%)\n",
      "  LOC_002: ₹3,479,302 (24.2%)\n",
      "  LOC_003: ₹2,851,810 (19.8%)\n",
      "  LOC_004: ₹2,183,037 (15.2%)\n",
      "  LOC_005: ₹811,974 (5.6%)\n",
      "\n",
      "================================================================================\n",
      "\n",
      "5. SUPPLIER ANALYSIS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Supplier Performance:\n",
      "  PowerCell Batteries Ltd:\n",
      "    Sales: ₹6,197,440\n",
      "    Rating: 4.2/5.0\n",
      "    Reliability: 0.88\n",
      "  BrakeTech Systems:\n",
      "    Sales: ₹3,945,922\n",
      "    Rating: 4.7/5.0\n",
      "    Reliability: 0.95\n",
      "  AutoParts Direct India:\n",
      "    Sales: ₹1,321,077\n",
      "    Rating: 4.5/5.0\n",
      "    Reliability: 0.92\n",
      "  SparkLife Components:\n",
      "    Sales: ₹943,712\n",
      "    Rating: 4.3/5.0\n",
      "    Reliability: 0.90\n",
      "  CoolFlow Systems:\n",
      "    Sales: ₹823,919\n",
      "    Rating: 4.1/5.0\n",
      "    Reliability: 0.87\n",
      "  VisionClear Wipers:\n",
      "    Sales: ₹688,181\n",
      "    Rating: 4.6/5.0\n",
      "    Reliability: 0.93\n",
      "  SuspensionPro India:\n",
      "    Sales: ₹475,979\n",
      "    Rating: 4.0/5.0\n",
      "    Reliability: 0.85\n",
      "\n",
      "================================================================================\n",
      "\n",
      "6. STATISTICAL SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Quantity Sold Distribution:\n",
      "  Mean: 1.26 units\n",
      "  Median: 1.00 units\n",
      "  Std Dev: 0.55\n",
      "  Min: 1\n",
      "  Max: 7\n",
      "\n",
      "Sales Value Distribution:\n",
      "  Mean: ₹1515.39\n",
      "  Median: ₹742.00\n",
      "  Std Dev: ₹2186.38\n",
      "  Min: ₹244.00\n",
      "  Max: ₹39739.00\n",
      "\n",
      "================================================================================\n",
      "\n",
      "7. FEATURE ENGINEERING SUMMARY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Original Features: 50\n",
      "Engineered Features: 117\n",
      "Encoded Features (Model-Ready): 129\n",
      "\n",
      "Feature Categories:\n",
      "  - Temporal features: 27\n",
      "  - Lag features: 4\n",
      "  - Rolling window features: 12\n",
      "  - Inventory features: 9\n",
      "  - Price/Cost features: 12\n",
      "  - Supplier features: 16\n",
      "  - Demand pattern features: 7\n",
      "  - Categorical features: 16\n",
      "\n",
      "================================================================================\n",
      "\n",
      "8. MODEL PREPARATION\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Target Variable: quantity_sold\n",
      "Number of Features: 117\n",
      "\n",
      "Train-Test Split (Time-based):\n",
      "  Training samples: 7,600 (80.0%)\n",
      "  Test samples: 1,900 (20.0%)\n",
      "  Train date range: 2022-01-01 00:00:00 to 2024-05-15 00:00:00\n",
      "  Test date range: 2024-05-15 00:00:00 to 2024-12-31 00:00:00\n",
      "\n",
      "================================================================================\n",
      "\n",
      "9. DATA QUALITY\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Missing Values: 0\n",
      "Duplicate Transactions: 0\n",
      "\n",
      "Data Completeness:\n",
      "  Sales Data: 100%\n",
      "  Parts Master: 100%\n",
      "  Supplier Data: 100%\n",
      "  Inventory Data: Available\n",
      "\n",
      "================================================================================\n",
      "\n",
      "10. KEY INSIGHTS\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "• Busiest Day: Wednesday\n",
      "• Best Selling Category: Electrical\n",
      "• Most Profitable SKU: SKU_ELC_001\n",
      "• Best Performing Location: LOC_001\n",
      "• Holiday Impact on Sales: +14.9%\n",
      "• Event Impact on Sales: -9.8%\n",
      "• Weekend Impact on Sales: -3.7%\n",
      "\n",
      "================================================================================\n",
      "END OF REPORT\n",
      "================================================================================\n",
      "\n",
      "✓ Summary report saved: processed_data\\EDA_Summary_Report.txt\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive summary report\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"GENERATING SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create report\n",
    "report_lines = []\n",
    "report_lines.append(\"=\" * 80)\n",
    "report_lines.append(\"AUTOMOTIVE SPARE PARTS - EDA & FEATURE ENGINEERING REPORT\")\n",
    "report_lines.append(\"=\" * 80)\n",
    "report_lines.append(f\"\\nReport Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "\n",
    "# 1. DATA OVERVIEW\n",
    "report_lines.append(\"\\n1. DATA OVERVIEW\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"Total Transactions: {len(df):,}\")\n",
    "report_lines.append(f\"Date Range: {df['date'].min()} to {df['date'].max()}\")\n",
    "report_lines.append(f\"Number of SKUs: {df['part_sku'].nunique()}\")\n",
    "report_lines.append(f\"Number of Locations: {df['location_id'].nunique()}\")\n",
    "report_lines.append(f\"Number of Suppliers: {df['supplier_id'].nunique()}\")\n",
    "report_lines.append(f\"\\nTotal Quantity Sold: {df['quantity_sold'].sum():,} units\")\n",
    "report_lines.append(f\"Total Sales Value: ₹{df['total_sales'].sum():,.2f}\")\n",
    "report_lines.append(f\"Average Transaction Value: ₹{df['total_sales'].mean():.2f}\")\n",
    "\n",
    "# 2. TEMPORAL ANALYSIS\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n2. TEMPORAL ANALYSIS\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "\n",
    "yearly_summary = df.groupby('year').agg({\n",
    "    'quantity_sold': 'sum',\n",
    "    'total_sales': 'sum',\n",
    "    'transaction_id': 'count'\n",
    "})\n",
    "report_lines.append(\"\\nSales by Year:\")\n",
    "for year, row in yearly_summary.iterrows():\n",
    "    report_lines.append(f\"  {year}: {row['quantity_sold']:,} units | ₹{row['total_sales']:,.0f} | {row['transaction_id']:,} transactions\")\n",
    "\n",
    "report_lines.append(\"\\nWeekend vs Weekday Sales:\")\n",
    "weekend_summary = df.groupby('is_weekend')['total_sales'].sum()\n",
    "weekday_pct = (weekend_summary[0] / weekend_summary.sum() * 100)\n",
    "weekend_pct = (weekend_summary[1] / weekend_summary.sum() * 100)\n",
    "report_lines.append(f\"  Weekday: ₹{weekend_summary[0]:,.0f} ({weekday_pct:.1f}%)\")\n",
    "report_lines.append(f\"  Weekend: ₹{weekend_summary[1]:,.0f} ({weekend_pct:.1f}%)\")\n",
    "\n",
    "# 3. PRODUCT ANALYSIS\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n3. PRODUCT ANALYSIS\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "\n",
    "report_lines.append(\"\\nTop 5 SKUs by Sales Value:\")\n",
    "top_skus = df.groupby('part_sku')['total_sales'].sum().sort_values(ascending=False).head(5)\n",
    "for idx, (sku, sales) in enumerate(top_skus.items(), 1):\n",
    "    report_lines.append(f\"  {idx}. {sku}: ₹{sales:,.0f}\")\n",
    "\n",
    "report_lines.append(\"\\nSales by Category:\")\n",
    "cat_summary = df.groupby('category')['total_sales'].sum().sort_values(ascending=False)\n",
    "for cat, sales in cat_summary.items():\n",
    "    pct = (sales / cat_summary.sum() * 100)\n",
    "    report_lines.append(f\"  {cat}: ₹{sales:,.0f} ({pct:.1f}%)\")\n",
    "\n",
    "report_lines.append(\"\\nCritical vs Non-Critical Spares:\")\n",
    "critical_summary = df.groupby('critical_spare')['total_sales'].sum()\n",
    "report_lines.append(f\"  Non-Critical: ₹{critical_summary[False]:,.0f} ({critical_summary[False]/critical_summary.sum()*100:.1f}%)\")\n",
    "report_lines.append(f\"  Critical: ₹{critical_summary[True]:,.0f} ({critical_summary[True]/critical_summary.sum()*100:.1f}%)\")\n",
    "\n",
    "# 4. LOCATION ANALYSIS\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n4. LOCATION ANALYSIS\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(\"\\nSales by Location:\")\n",
    "loc_summary = df.groupby('location_id')['total_sales'].sum().sort_values(ascending=False)\n",
    "for loc, sales in loc_summary.items():\n",
    "    pct = (sales / loc_summary.sum() * 100)\n",
    "    report_lines.append(f\"  {loc}: ₹{sales:,.0f} ({pct:.1f}%)\")\n",
    "\n",
    "# 5. SUPPLIER ANALYSIS\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n5. SUPPLIER ANALYSIS\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "supplier_summary = df.groupby('supplier_name').agg({\n",
    "    'total_sales': 'sum',\n",
    "    'supplier_rating': 'first',\n",
    "    'reliability_score': 'first'\n",
    "}).sort_values('total_sales', ascending=False)\n",
    "\n",
    "report_lines.append(\"\\nSupplier Performance:\")\n",
    "for supplier, row in supplier_summary.iterrows():\n",
    "    report_lines.append(f\"  {supplier}:\")\n",
    "    report_lines.append(f\"    Sales: ₹{row['total_sales']:,.0f}\")\n",
    "    report_lines.append(f\"    Rating: {row['supplier_rating']:.1f}/5.0\")\n",
    "    report_lines.append(f\"    Reliability: {row['reliability_score']:.2f}\")\n",
    "\n",
    "# 6. STATISTICAL SUMMARY\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n6. STATISTICAL SUMMARY\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(\"\\nQuantity Sold Distribution:\")\n",
    "report_lines.append(f\"  Mean: {df['quantity_sold'].mean():.2f} units\")\n",
    "report_lines.append(f\"  Median: {df['quantity_sold'].median():.2f} units\")\n",
    "report_lines.append(f\"  Std Dev: {df['quantity_sold'].std():.2f}\")\n",
    "report_lines.append(f\"  Min: {df['quantity_sold'].min()}\")\n",
    "report_lines.append(f\"  Max: {df['quantity_sold'].max()}\")\n",
    "\n",
    "report_lines.append(\"\\nSales Value Distribution:\")\n",
    "report_lines.append(f\"  Mean: ₹{df['total_sales'].mean():.2f}\")\n",
    "report_lines.append(f\"  Median: ₹{df['total_sales'].median():.2f}\")\n",
    "report_lines.append(f\"  Std Dev: ₹{df['total_sales'].std():.2f}\")\n",
    "report_lines.append(f\"  Min: ₹{df['total_sales'].min():.2f}\")\n",
    "report_lines.append(f\"  Max: ₹{df['total_sales'].max():.2f}\")\n",
    "\n",
    "# 7. FEATURE ENGINEERING SUMMARY\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n7. FEATURE ENGINEERING SUMMARY\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"\\nOriginal Features: {len(df.columns)}\")\n",
    "report_lines.append(f\"Engineered Features: {len(df_fe.columns)}\")\n",
    "report_lines.append(f\"Encoded Features (Model-Ready): {len(df_encoded.columns)}\")\n",
    "report_lines.append(f\"\\nFeature Categories:\")\n",
    "report_lines.append(f\"  - Temporal features: {len(temporal_features)}\")\n",
    "report_lines.append(f\"  - Lag features: {len(lag_features)}\")\n",
    "report_lines.append(f\"  - Rolling window features: {len(rolling_features)}\")\n",
    "report_lines.append(f\"  - Inventory features: {len(inventory_features)}\")\n",
    "report_lines.append(f\"  - Price/Cost features: {len(price_features)}\")\n",
    "report_lines.append(f\"  - Supplier features: {len(supplier_features)}\")\n",
    "report_lines.append(f\"  - Demand pattern features: {len(demand_features)}\")\n",
    "report_lines.append(f\"  - Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# 8. MODEL PREPARATION\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n8. MODEL PREPARATION\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"\\nTarget Variable: {target}\")\n",
    "report_lines.append(f\"Number of Features: {len(feature_cols)}\")\n",
    "report_lines.append(f\"\\nTrain-Test Split (Time-based):\")\n",
    "report_lines.append(f\"  Training samples: {X_train.shape[0]:,} ({X_train.shape[0]/len(df_encoded)*100:.1f}%)\")\n",
    "report_lines.append(f\"  Test samples: {X_test.shape[0]:,} ({X_test.shape[0]/len(df_encoded)*100:.1f}%)\")\n",
    "report_lines.append(f\"  Train date range: {train_data['date'].min()} to {train_data['date'].max()}\")\n",
    "report_lines.append(f\"  Test date range: {test_data['date'].min()} to {test_data['date'].max()}\")\n",
    "\n",
    "# 9. DATA QUALITY\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n9. DATA QUALITY\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "report_lines.append(f\"\\nMissing Values: {df_encoded.isnull().sum().sum()}\")\n",
    "report_lines.append(f\"Duplicate Transactions: {df['transaction_id'].duplicated().sum()}\")\n",
    "report_lines.append(f\"\\nData Completeness:\")\n",
    "report_lines.append(f\"  Sales Data: 100%\")\n",
    "report_lines.append(f\"  Parts Master: 100%\")\n",
    "report_lines.append(f\"  Supplier Data: 100%\")\n",
    "report_lines.append(f\"  Inventory Data: Available\")\n",
    "\n",
    "# 10. KEY INSIGHTS\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"\\n10. KEY INSIGHTS\")\n",
    "report_lines.append(\"-\" * 80)\n",
    "\n",
    "# Busiest day\n",
    "busiest_day = df.groupby('day_name')['transaction_id'].count().idxmax()\n",
    "report_lines.append(f\"\\n• Busiest Day: {busiest_day}\")\n",
    "\n",
    "# Best selling category\n",
    "best_category = df.groupby('category')['total_sales'].sum().idxmax()\n",
    "report_lines.append(f\"• Best Selling Category: {best_category}\")\n",
    "\n",
    "# Most profitable SKU\n",
    "most_profitable = df.groupby('part_sku')['total_sales'].sum().idxmax()\n",
    "report_lines.append(f\"• Most Profitable SKU: {most_profitable}\")\n",
    "\n",
    "# Best performing location\n",
    "best_location = df.groupby('location_id')['total_sales'].sum().idxmax()\n",
    "report_lines.append(f\"• Best Performing Location: {best_location}\")\n",
    "\n",
    "# Holiday impact\n",
    "holiday_avg = df[df['is_holiday']]['total_sales'].mean()\n",
    "non_holiday_avg = df[~df['is_holiday']]['total_sales'].mean()\n",
    "holiday_impact = ((holiday_avg - non_holiday_avg) / non_holiday_avg * 100)\n",
    "report_lines.append(f\"• Holiday Impact on Sales: {holiday_impact:+.1f}%\")\n",
    "\n",
    "# Event impact\n",
    "event_avg = df[df['is_event']]['total_sales'].mean()\n",
    "non_event_avg = df[~df['is_event']]['total_sales'].mean()\n",
    "event_impact = ((event_avg - non_event_avg) / non_event_avg * 100)\n",
    "report_lines.append(f\"• Event Impact on Sales: {event_impact:+.1f}%\")\n",
    "\n",
    "# Weekend impact\n",
    "weekend_avg = df[df['is_weekend'] == 1]['total_sales'].mean()\n",
    "weekday_avg = df[df['is_weekend'] == 0]['total_sales'].mean()\n",
    "weekend_impact = ((weekend_avg - weekday_avg) / weekday_avg * 100)\n",
    "report_lines.append(f\"• Weekend Impact on Sales: {weekend_impact:+.1f}%\")\n",
    "\n",
    "report_lines.append(f\"\\n{'=' * 80}\")\n",
    "report_lines.append(\"END OF REPORT\")\n",
    "report_lines.append(\"=\" * 80)\n",
    "\n",
    "# Save report to file\n",
    "report_file = os.path.join(output_dir, 'EDA_Summary_Report.txt')\n",
    "with open(report_file, 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(report_lines))\n",
    "\n",
    "# Print report to console\n",
    "print('\\n'.join(report_lines))\n",
    "\n",
    "print(f\"\\n✓ Summary report saved: {report_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bb34f03-2758-4245-b8a5-459f0cacfc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Metadata saved: processed_data\\metadata.json\n",
      "\n",
      "================================================================================\n",
      "ALL FILES SAVED SUCCESSFULLY!\n",
      "================================================================================\n",
      "\n",
      "Output directory: C:\\Users\\Admin\\Desktop\\Spare_parts\\processed_data\n",
      "\n",
      "Generated files:\n",
      "  ✓ feature_engineered_dataset.csv (9328.0 KB)\n",
      "  ✓ encoded_dataset_model_ready.csv (9201.8 KB)\n",
      "  ✓ X_train.csv (6426.1 KB)\n",
      "  ✓ y_train.csv (22.3 KB)\n",
      "  ✓ X_test.csv (1616.3 KB)\n",
      "  ✓ y_test.csv (5.6 KB)\n",
      "  ✓ feature_information.csv (3.0 KB)\n",
      "  ✓ sku_statistics.csv (0.9 KB)\n",
      "  ✓ location_statistics.csv (0.5 KB)\n",
      "  ✓ sku_location_statistics.csv (3.5 KB)\n",
      "  ✓ EDA_Summary_Report.txt (5.5 KB)\n",
      "  ✓ metadata.json (1.3 KB)\n"
     ]
    }
   ],
   "source": [
    "# Create a metadata file with processing information\n",
    "\n",
    "metadata = {\n",
    "    'processing_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_data': {\n",
    "        'sales_transactions': len(df),\n",
    "        'date_range_start': str(df['date'].min()),\n",
    "        'date_range_end': str(df['date'].max()),\n",
    "        'unique_skus': df['part_sku'].nunique(),\n",
    "        'unique_locations': df['location_id'].nunique(),\n",
    "        'unique_suppliers': df['supplier_id'].nunique()\n",
    "    },\n",
    "    'feature_engineering': {\n",
    "        'original_features': len(df.columns),\n",
    "        'engineered_features': len(df_fe.columns),\n",
    "        'encoded_features': len(df_encoded.columns),\n",
    "        'final_model_features': len(feature_cols)\n",
    "    },\n",
    "    'train_test_split': {\n",
    "        'split_method': 'time-based',\n",
    "        'train_samples': int(X_train.shape[0]),\n",
    "        'test_samples': int(X_test.shape[0]),\n",
    "        'train_percentage': float(X_train.shape[0]/len(df_encoded)*100),\n",
    "        'test_percentage': float(X_test.shape[0]/len(df_encoded)*100)\n",
    "    },\n",
    "    'feature_categories': {\n",
    "        'temporal': len(temporal_features),\n",
    "        'lag': len(lag_features),\n",
    "        'rolling_window': len(rolling_features),\n",
    "        'inventory': len(inventory_features),\n",
    "        'price_cost': len(price_features),\n",
    "        'supplier': len(supplier_features),\n",
    "        'demand_pattern': len(demand_features),\n",
    "        'categorical': len(categorical_features)\n",
    "    },\n",
    "    'files_generated': [\n",
    "        'feature_engineered_dataset.csv',\n",
    "        'encoded_dataset_model_ready.csv',\n",
    "        'X_train.csv',\n",
    "        'y_train.csv',\n",
    "        'X_test.csv',\n",
    "        'y_test.csv',\n",
    "        'feature_information.csv',\n",
    "        'sku_statistics.csv',\n",
    "        'location_statistics.csv',\n",
    "        'sku_location_statistics.csv',\n",
    "        'EDA_Summary_Report.txt',\n",
    "        'metadata.json'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "import json\n",
    "metadata_file = os.path.join(output_dir, 'metadata.json')\n",
    "with open(metadata_file, 'w') as f:\n",
    "    json.dump(metadata, f, indent=4)\n",
    "\n",
    "print(f\"\\n✓ Metadata saved: {metadata_file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALL FILES SAVED SUCCESSFULLY!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nOutput directory: {os.path.abspath(output_dir)}\")\n",
    "print(f\"\\nGenerated files:\")\n",
    "for file in metadata['files_generated']:\n",
    "    file_path = os.path.join(output_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"  ✓ {file} ({size:.1f} KB)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c73ed1-6237-4f39-8607-a420cec0be2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
